from scipy.sparse import diags
from scipy.integrate import odeint
import numpy as np
import matplotlib.pyplot as plt


def f(u):
    return -u * (1. - u) * (0.3 - u)


def pde(u, t, dx):
    dudt = np.empty_like(u)
    dudt[0] = f(u[0]) + (-2.0 * u[0] + 2.0 * u[1]) / dx ** 2
    dudt[1:-1] = f(u[1:-1]) + np.diff(u, 2) / dx ** 2
    dudt[-1] = f(u[-1]) + (-2.0 * u[-1] + 2.0 * u[-1]) / dx ** 2

    return dudt


h = 100.
x = np.linspace(0., 100., num=h)
dx = len(x) / h

init_cond = [0] * (len(x))
for i in range(int(round(0.2 * len(init_cond)))):
    init_cond[i] = 1.
# print(init_cond)

t = np.linspace(0, 125, 125)

sol = odeint(pde, init_cond, t, args=(dx,), ml=1, mu=1)
# print(sol)

plt.plot(x, sol[-1])
plt.show()
# plt.plot(x, np.sin(x))
# plt.show()
# print(init_cond)
